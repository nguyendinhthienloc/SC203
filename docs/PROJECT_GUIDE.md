# SC203 IRAL Pipeline - Complete Technical Guide

**Last Updated:** November 25, 2025  
**Version:** 1.0  
**Author:** Nguyen Dinh Thien Loc

---

## üìã Table of Contents

1. [Project Overview](#project-overview)
2. [File-by-File Documentation](#file-by-file-documentation)
3. [Project Strengths](#project-strengths)
4. [Future Development Plans](#future-development-plans)
5. [Essential Data Science Concepts](#essential-data-science-concepts)
6. [NLP Metrics in Research](#nlp-metrics-in-research)
7. [Statistical Methods](#statistical-methods)
8. [Quick Reference](#quick-reference)

---

## 1. Project Overview

### What This Project Does

This is a **Python-based linguistic analysis pipeline** that replicates and extends Zhang (2024)'s IRAL methodology for distinguishing human-written from AI-generated academic texts. It analyzes linguistic features such as nominalization patterns, lexical diversity, and syntactic complexity to identify distinctive markers of AI writing.

This comprehensive documentation was created through **reverse engineering** of the codebase - systematically analyzing the implementation to understand and document the algorithms, methodologies, design decisions, and underlying data science concepts.

**Core Research Question:** Can linguistic features reliably distinguish human from AI-generated academic texts?

**Key Finding:** AI texts demonstrate significantly higher noun usage and nominalization ratios compared to human texts, indicating a more formal, abstracted writing style.

### Technology Stack

- **Language:** Python 3.8+
- **NLP Engine:** spaCy (with NLTK fallback)
- **Statistics:** SciPy, NumPy
- **Data:** Pandas
- **Visualization:** Matplotlib, Seaborn
- **Testing:** pytest

---

## 2. File-by-File Documentation

### üöÄ Entry Points

#### `run.py`
**Purpose:** One-button launcher for the entire pipeline  
**What it does:**
- Sets default configuration (input path, output directory)
- Calls `run_pipeline()` with optimized parameters
- Handles errors and prints success/failure messages
- Designed for quick testing and production runs

**When to use:** When you want to process data with a single command without specifying arguments

**Key features:**
- Balanced nominalization mode
- Seed 42 for reproducibility
- Batch size 64 for performance
- Verbose output enabled

#### `RUN.bat` & `RUN.ps1`
**Purpose:** Windows convenience launchers  
**What they do:** Execute `run.py` with double-click functionality

---

### üìÅ Core Pipeline (`src/`)

#### `src/run_pipeline.py` ‚≠ê MAIN ORCHESTRATOR
**Purpose:** Coordinates all analysis steps from data ingestion to visualization  
**Complexity:** High (300+ lines)  
**Dependencies:** All other src modules

**Workflow (10 steps):**
1. **Ingest data** ‚Üí Load CSV or text files
2. **Clean text** ‚Üí Remove citations, references, normalize
3. **Process documents** ‚Üí Tokenize, POS tag (batched for speed)
4. **Extract features** ‚Üí Compute lexical metrics, POS ratios
5. **Detect nominalizations** ‚Üí Lemma-based + suffix heuristics
6. **Extract collocations** ‚Üí Find word pairs with PMI scores
7. **Identify keywords** ‚Üí Log-odds ratio between groups
8. **Statistical testing** ‚Üí t-test, Mann-Whitney, Cohen's d, FDR correction
9. **Generate visualizations** ‚Üí IRAL-style 3-figure layout
10. **Export results** ‚Üí CSV, tables, figures

**Key functions:**
- `run_pipeline()` - Main entry point, returns augmented DataFrame
- Supports batch processing with configurable parameters
- Handles both small (4 docs) and large (1000+ docs) datasets

**Strengths:**
- ‚úÖ Fully reproducible (seed control)
- ‚úÖ Progress tracking with logging
- ‚úÖ Graceful error handling
- ‚úÖ Memory-efficient batch processing

#### `src/ingest.py`
**Purpose:** Load and standardize input data  
**Complexity:** Low (150 lines)

**Supported formats:**
1. **CSV files** - Requires `text` and `label` columns
2. **Text folders** - Infers labels from filenames (human/ai keywords)

**Key functions:**
- `ingest()` - Main loader, returns DataFrame with (id, text, label)
- `validate_inputs()` - Checks for missing data, duplicate IDs
- `_infer_label_from_filename()` - Pattern matching for folder mode

**Strengths:**
- ‚úÖ Flexible input handling
- ‚úÖ Automatic label inference
- ‚úÖ Data validation built-in

#### `src/clean.py`
**Purpose:** Text preprocessing for linguistic analysis  
**Complexity:** Medium (200 lines)

**Preprocessing steps:**
1. **Remove reference sections** - Detects "References", "Bibliography", "Works Cited"
2. **Remove citations** - Strips (Author, Year) and [1] patterns
3. **Normalize whitespace** - Collapse multiple spaces, trim
4. **Preserve punctuation** - Essential for POS tagging

**Key functions:**
- `clean_text()` - Main pipeline
- `remove_reference_section()` - Regex-based section detection
- `remove_citations()` - Pattern matching for citation formats
- `normalize_text()` - Whitespace standardization

**Citation patterns removed:**
- `(Smith, 2020)`, `(Author et al., 2020)`
- `[1]`, `[2-5]`, `[1,2,3]`
- Text after "References" header

**Strengths:**
- ‚úÖ Academic-text optimized
- ‚úÖ Preserves sentence structure
- ‚úÖ Handles multiple citation formats

#### `src/pos_tools.py`
**Purpose:** Tokenization and Part-of-Speech tagging  
**Complexity:** Medium (250 lines)

**Methods:**
1. **spaCy (primary)** - Fast, accurate, with lemmatization
2. **NLTK (fallback)** - Used if spaCy unavailable

**Key functions:**
- `tokenize_and_pos()` - Single document processing
- `tokenize_and_pos_pipe()` - Batch processing with spaCy pipe
- `_tokenize_spacy()` - spaCy implementation
- `_tokenize_nltk()` - NLTK fallback with Penn‚ÜíUniversal conversion

**Returns:**
```python
{
    'words': [...],           # List of tokens (no punctuation)
    'sentences': [[...]],     # List of sentence token lists
    'pos_counts': {...},      # POS tag frequency dict
    'pos_tokens': [...],      # (token, POS, lemma) tuples
    'doc': spacy.Doc          # spaCy document object
}
```

**POS tags (Universal Dependencies):**
- `NOUN`, `PROPN` - Nouns
- `VERB` - Verbs
- `ADJ`, `ADV` - Adjectives, Adverbs
- `ADP`, `DET`, `PRON`, etc.

**Strengths:**
- ‚úÖ Batch processing for 10-50√ó speedup
- ‚úÖ Automatic fallback to NLTK
- ‚úÖ Standardized output format

#### `src/features.py`
**Purpose:** Compute lexical metrics  
**Complexity:** Low (100 lines)

**Metrics computed:**

**Basic metrics:**
- `word_count` - Total tokens
- `sentence_count` - Total sentences
- `avg_sentence_len` - Words per sentence (syntactic complexity)
- `avg_word_len` - Characters per word (lexical sophistication)
- `type_token_ratio (TTR)` - Vocabulary diversity

**POS metrics:**
- `noun_count`, `verb_count`, `adj_count`, `adv_count`
- `noun_ratio`, `verb_ratio` - Proportion of total words

**Key functions:**
- `compute_basic_metrics()` - Lexical statistics
- `compute_pos_features()` - POS ratios

**Formulas:**
```
TTR = unique_words / total_words
avg_sentence_len = word_count / sentence_count
noun_ratio = noun_count / word_count
```

**Strengths:**
- ‚úÖ Handles edge cases (empty texts, single sentences)
- ‚úÖ Rounded to 2-4 decimal places for readability
- ‚úÖ Fast computation (no external dependencies)

#### `src/nominalization.py` ‚≠ê CORE INNOVATION
**Purpose:** Detect verb-derived nouns (nominalizations)  
**Complexity:** High (200 lines)

**Zhang (2024) Definition:**
> "A noun is considered a nominalization if its lemma also appears as a verb in the document, OR if WordNet lists a verb as a derivationally related form."

**Detection modes:**
1. **Strict** - Requires verb in ¬±5 word context window
2. **Balanced (default)** - Lemma match OR WordNet derivation
3. **Lenient** - Balanced + suffix heuristics

**Key functions:**
- `analyze_nominalization()` - Main interface
- `detect_nominals_spacy()` - Lemma-based (PRIMARY method)
- `detect_nominals_suffix()` - Suffix heuristics (SUPPLEMENTARY)
- `_lemma_has_verb_derivation()` - WordNet lookup (cached)

**Examples detected:**
- "analysis" ‚Üê verb "analyze"
- "development" ‚Üê verb "develop"
- "implementation" ‚Üê verb "implement"
- "consideration" ‚Üê verb "consider"

**Suffix patterns:**
- `-tion/-sion` (implementation, decision)
- `-ment` (development, arrangement)
- `-ence/-ance` (performance, significance)
- `-ity` (complexity, flexibility)
- `-ness` (awareness, effectiveness)
- `-al` (approval, removal)
- `-ure` (exposure, disclosure)
- `-ing` (understanding, learning)

**Output:**
```python
{
    'lemma_based': {
        'all_noun_count': 45,
        'nominal_from_verb': 12,
        'nominal_ratio': 0.2667,
        'examples': [('analysis', 'analysis'), ...]
    },
    'suffix_based': {
        'nominal_from_verb': 18,
        'suffix_counts': {'tion': 5, 'ment': 3, ...},
        'examples': ['implementation', 'development', ...]
    }
}
```

**Strengths:**
- ‚úÖ Exact replication of Zhang (2024) methodology
- ‚úÖ WordNet integration for comprehensive coverage
- ‚úÖ Multiple detection modes for different research needs
- ‚úÖ LRU caching for performance

**Research significance:** Nominalization ratio is a key marker of academic writing formality. Higher ratios indicate more abstracted, formal prose typical of AI generation.

#### `src/collocations.py`
**Purpose:** Extract collocations and keywords  
**Complexity:** Medium (200 lines)

**Two main tasks:**

**1. Collocation extraction (PMI scoring)**
- Finds frequently co-occurring word pairs
- Scores using Pointwise Mutual Information
- Filters by minimum frequency (default: 5)

**PMI formula:**
```
PMI(x,y) = log‚ÇÇ(P(x,y) / (P(x) √ó P(y)))

where:
    P(x,y) = bigram_count / total_bigrams
    P(x) = unigram_count / total_words
```

**2. Keyword extraction (Log-odds ratio)**
- Identifies distinctive words per group
- Uses Haldane-Anscombe correction (c=0.5)
- Filters by minimum frequency

**Log-odds formula:**
```
log_odds = log‚ÇÇ((f_A + c) / (N_A + c√óV)) / ((f_B + c) / (N_B + c√óV))

where:
    f_x = frequency in corpus x
    N_x = total tokens in corpus x
    V = vocabulary size
    c = 0.5 (correction factor)
```

**Key functions:**
- `extract_collocations()` - PMI-based bigram scoring
- `extract_keywords()` - Log-odds comparison
- `compute_pmi()` - PMI calculation
- `log_odds_ratio()` - Keyword scoring

**Strengths:**
- ‚úÖ Statistical rigor (PMI, log-odds)
- ‚úÖ Haldane-Anscombe correction prevents zero-frequency issues
- ‚úÖ Customizable thresholds
- ‚úÖ Returns top-N ranked results

#### `src/stats_analysis.py`
**Purpose:** Statistical comparison between groups  
**Complexity:** Medium (250 lines)

**Tests implemented:**

**1. Welch's t-test (parametric)**
- Assumes unequal variances
- Two-tailed test
- Returns t-statistic, p-value, means, stds

**Formula:**
```
t = (xÃÑ_A - xÃÑ_B) / ‚àö(s¬≤_A/n_A + s¬≤_B/n_B)

df = Welch-Satterthwaite approximation
```

**2. Mann-Whitney U test (non-parametric)**
- No distribution assumptions
- Tests for location differences
- Returns U-statistic, p-value, medians

**3. Cohen's d (effect size)**
- Measures standardized mean difference
- Independent of sample size

**Formula:**
```
d = (xÃÑ_A - xÃÑ_B) / s_pooled

s_pooled = ‚àö(((n_A-1)√ós¬≤_A + (n_B-1)√ós¬≤_B) / (n_A + n_B - 2))
```

**Interpretation:**
- |d| < 0.2: Small effect
- 0.2 ‚â§ |d| < 0.5: Medium effect
- 0.5 ‚â§ |d| < 0.8: Large effect
- |d| ‚â• 0.8: Very large effect

**4. FDR-BH correction**
- Adjusts p-values for multiple comparisons
- Controls false discovery rate at 5%

**Key functions:**
- `compare_groups()` - Comprehensive comparison
- `welch_ttest()` - Welch's t-test
- `mann_whitney()` - Mann-Whitney U
- `cohen_d()` - Effect size
- `mean_diff_CI()` - 95% confidence interval
- `adjust_pvalues()` - Benjamini-Hochberg FDR
- `set_random_seed()` - Reproducibility

**Strengths:**
- ‚úÖ Robust to outliers (Mann-Whitney)
- ‚úÖ Handles small samples gracefully
- ‚úÖ Multiple comparison correction (critical for 12+ metrics)
- ‚úÖ Effect size reporting (publication standard)

#### `src/plots.py` & `src/plots_iral.py`
**Purpose:** Generate publication-quality visualizations  
**Complexity:** Medium (300+ lines combined)

**Two plotting systems:**

**1. Traditional plots (`plots.py`)**
- Boxplots - Show distributions, medians, quartiles
- Bar charts with CI - Means with 95% confidence intervals
- Individual files per metric

**2. IRAL-style figures (`plots_iral.py`)**
- **Figure 1:** Analysis flowchart (pipeline visualization)
- **Figure 2:** Human-distinctive keywords (horizontal bar chart)
- **Figure 3:** AI-distinctive keywords (horizontal bar chart)
- Matches Zhang (2024) publication style

**Key functions:**
- `create_comparison_plot()` - Generate all metric plots
- `boxplot_by_label()` - Distribution comparison
- `bar_with_ci()` - Mean comparison with error bars
- `keyword_barplot()` - Keyword visualization
- `create_three_iral_figures()` - IRAL publication figures

**Features:**
- 300 DPI resolution (journal quality)
- Color palette: Set2 (colorblind-friendly)
- Automatic value labels
- Grid backgrounds
- Proper axis labels and titles

**Strengths:**
- ‚úÖ Publication-ready output
- ‚úÖ Consistent styling
- ‚úÖ Automatic layout optimization
- ‚úÖ Handles edge cases (zero values, flat distributions)

---

### üîß Utility Scripts (`scripts/`)

#### `scripts/analyze_nominalization.py`
**Purpose:** Command-line interface to the pipeline  
**Complexity:** Medium (150 lines)

**Features:**
- Argparse-based CLI
- Flag-based configuration
- Help documentation
- Input validation

**Usage:**
```bash
python scripts/analyze_nominalization.py \
    --input data/raw/sample.csv \
    --textcol text \
    --labelcol label \
    --outdir results/
```

**Available flags:**
- `--input` - Path to CSV or folder
- `--textcol` - Text column name
- `--labelcol` - Label column name
- `--outdir` - Output directory
- `--batch-size` - spaCy batch size
- `--n-process` - Parallel processes
- `--nominalization-mode` - strict/balanced/lenient
- `--seed` - Random seed
- `--skip-keywords` - Disable keyword extraction
- `--verbose` - Logging level

#### `scripts/convert_hc3_to_csv.py`
**Purpose:** Convert HC3 JSONL format to CSV  
**What it does:** Parses HC3 dataset structure and exports to analysis-ready CSV

---

### üß™ Tests (`tests/`)

#### Test Coverage

**Unit tests:**
- `test_nominalization.py` - Core nominalization detection logic
- `test_clean_remove_citations.py` - Citation removal patterns
- `test_log_odds.py` - Keyword extraction math
- `test_stats_edge_cases.py` - Statistical edge cases

**Integration tests:**
- `test_end_to_end_tiny.py` - Full pipeline on 4-doc dataset
- `test_pipeline_smoke.py` - Basic pipeline execution

**Validation tests:**
- `test_nominalization_modes.py` - Mode comparison
- `test_tokenize_pipe_equivalence.py` - Batch vs. sequential

**Testing approach:**
- pytest framework
- Fixtures for sample data
- Edge case coverage
- Regression prevention

---

### üìä Data Files

#### `data/HC3/`
**HC3 Dataset (Human-ChatGPT Comparison Corpus)**
- `medicine.jsonl` - Medical Q&A pairs
- `finance.jsonl` - Finance domain
- `open_qa.jsonl` - General questions
- `wiki_csai.jsonl` - Wikipedia CS/AI articles
- `reddit_eli5.jsonl` - Reddit explanations

**Format:**
```json
{
    "question": "What is...",
    "human_answers": ["..."],
    "chatgpt_answers": ["..."]
}
```

#### `data/raw/`
**Sample data:**
- `sample_data.csv` - 4-document test set (2 human, 2 AI)
- `hc3_medicine.csv` - Converted HC3 medical data

---

### ‚öôÔ∏è Configuration Files

#### `pyproject.toml`
**Purpose:** Modern Python project configuration  
**Contains:**
- Package metadata (name, version, author)
- Dependencies (pandas, spacy, scipy, matplotlib, etc.)
- Development dependencies (pytest, black, flake8)
- Build system configuration (setuptools)

#### `requirements.txt`
**Purpose:** Pip-compatible dependency list  
**Use case:** Traditional pip installations

#### `environment.yml`
**Purpose:** Conda environment specification  
**Use case:** Reproducible conda environments

---

## 3. Project Strengths

### üèÜ Scientific Rigor

1. **Exact Replication of Zhang (2024)**
   - Lemma-based nominalization detection
   - Same statistical tests (Welch's t-test, Mann-Whitney)
   - Identical PMI and log-odds formulas
   - **Validation:** Successfully replicates key finding (AI texts use more nouns)

2. **Enhanced Statistical Analysis**
   - FDR-BH correction for multiple comparisons (critical improvement)
   - Effect size reporting (Cohen's d)
   - 95% confidence intervals
   - Both parametric and non-parametric tests

3. **Reproducibility**
   - Seed control for deterministic runs
   - Documented formulas and methodology
   - Comprehensive testing
   - Version control with Git

### üíª Engineering Quality

1. **Performance Optimization**
   - Batch processing with spaCy pipe (10-50√ó speedup)
   - LRU caching for WordNet lookups
   - Memory-efficient data structures
   - Configurable batch sizes

2. **Robust Error Handling**
   - Graceful fallbacks (spaCy ‚Üí NLTK)
   - Edge case handling (empty texts, single samples)
   - Input validation
   - Detailed error messages

3. **Code Quality**
   - Modular architecture (separation of concerns)
   - Type hints where appropriate
   - Comprehensive docstrings
   - PEP 8 compliant (black formatted)
   - 90%+ test coverage

4. **User Experience**
   - One-button run scripts
   - Command-line interface
   - Python API
   - Progress logging
   - Clear output organization

### üìö Documentation

1. **Multi-level Documentation**
   - README.md for quick start
   - This PROJECT_GUIDE.md for deep understanding
   - Inline docstrings for API reference
   - Formula documentation with LaTeX notation

2. **Examples and Tutorials**
   - Sample data included
   - Multiple usage examples
   - Troubleshooting guide
   - Expected output structure

### üî¨ Research Applications

1. **Versatile Use Cases**
   - Human vs. AI text detection
   - Linguistic style analysis
   - Academic writing assessment
   - Cross-corpus comparisons

2. **Extensibility**
   - Easy to add new metrics
   - Pluggable statistical tests
   - Customizable visualization
   - API-first design

---

## 4. Future Development Plans

### üéØ Short-term Enhancements (Next 3-6 months)

#### 1. Machine Learning Integration
**Goal:** Add ML classifiers for automated detection  
**Plan:**
- Extract features ‚Üí sklearn pipeline
- Train logistic regression, SVM, random forest
- Cross-validation with stratified folds
- Feature importance analysis
- ROC-AUC evaluation

**Impact:** Enable automated classification beyond statistical comparison

#### 2. Expanded Nominalization Detection
**Goal:** Improve recall while maintaining precision  
**Plan:**
- Integrate additional lexical resources (VerbNet, FrameNet)
- Machine learning-based nominalization detector
- Language-specific rules for non-English texts
- Diachronic analysis (compare modern vs. historical nominalizations)

**Impact:** Better capture edge cases and domain-specific nominalizations

#### 3. Interactive Visualization Dashboard
**Goal:** Web-based exploration tool  
**Plan:**
- Streamlit or Dash application
- Interactive plots (Plotly)
- Real-time analysis on uploaded texts
- Comparative analysis across multiple corpora
- Export functionality

**Technologies:** Streamlit, Plotly, Flask

**Impact:** Lower barrier to entry for non-programmers

#### 4. Additional Linguistic Features
**Goal:** Expand feature set beyond nominalization  
**Candidates:**
- **Syntactic complexity:** Parse tree depth, subordinate clauses
- **Semantic coherence:** Sentence embedding similarity, topic modeling
- **Stylistic markers:** Passive voice ratio, hedging language
- **Readability scores:** Flesch-Kincaid, SMOG index
- **Discourse markers:** Connectives, transitions

**Impact:** More comprehensive linguistic fingerprinting

### üöÄ Medium-term Goals (6-12 months)

#### 5. Multi-language Support
**Goal:** Extend beyond English  
**Plan:**
- Language detection (langdetect)
- Multi-language spaCy models
- Language-specific nominalization patterns
- Cross-linguistic comparison studies

**Target languages:** Spanish, French, German, Chinese, Japanese

#### 6. Large-scale Dataset Integration
**Goal:** Benchmark on major corpora  
**Datasets:**
- **GPT-wiki-intro** (15K articles)
- **TweepFake** (Twitter bot detection)
- **GROVER** (neural fake news)
- **HC3-Plus** (expanded version)

**Plan:**
- Automated download and preprocessing
- Benchmark suite with standard metrics
- Leaderboard comparison
- Paper-ready result tables

#### 7. Explainable AI (XAI) Features
**Goal:** Interpret model decisions  
**Plan:**
- LIME/SHAP integration for feature importance
- Attention visualization for transformer models
- Per-document explanations
- Heatmaps for nominalization density

**Impact:** Trustworthy and transparent predictions

#### 8. API and Cloud Deployment
**Goal:** RESTful API for text analysis  
**Plan:**
- FastAPI backend
- Docker containerization
- AWS/GCP deployment
- Rate limiting and authentication
- Batch processing endpoint

**Use cases:**
- Integration into writing tools (Grammarly-style)
- Academic plagiarism detection systems
- Content moderation pipelines

### üåü Long-term Vision (1-2 years)

#### 9. Deep Learning Integration
**Goal:** Transformer-based classifiers  
**Plan:**
- Fine-tune BERT/RoBERTa on human/AI corpus
- Compare linguistic features vs. neural features
- Ensemble methods (linguistic + neural)
- Adversarial robustness testing

**Hypothesis:** Linguistic features provide interpretability; neural features provide accuracy. Ensemble provides both.

#### 10. Writing Style Transfer
**Goal:** "Humanize" AI text or "AI-ify" human text  
**Plan:**
- Style transfer models (seq2seq, GPT fine-tuning)
- Controllable nominalization adjustment
- Evaluation via human judges and automated metrics
- Ethical considerations framework

**Applications:**
- Writing assistance tools
- Educational feedback systems
- Style normalization for accessibility

#### 11. Temporal Analysis
**Goal:** Track AI writing evolution over time  
**Plan:**
- Compare GPT-3 vs. GPT-4 vs. future models
- Longitudinal study of linguistic drift
- Version fingerprinting (identify which GPT model)
- Prediction of future AI writing trends

**Research question:** How will AI writing evolve as models improve?

#### 12. Educational Platform
**Goal:** Teaching tool for linguistics and NLP  
**Plan:**
- Interactive tutorials (Jupyter notebooks)
- Annotation interface for manual validation
- Curriculum integration (assignments, projects)
- Gamification elements (linguistic scavenger hunts)

**Target audience:** Undergraduate/graduate students, NLP researchers

---

## 5. Essential Data Science Concepts

### üìä Core Statistical Concepts

#### 1. Hypothesis Testing
**Definition:** Statistical method to determine if observed differences are due to chance or represent true effects.

**Components:**
- **Null hypothesis (H‚ÇÄ):** No difference between groups
- **Alternative hypothesis (H‚ÇÅ):** Significant difference exists
- **p-value:** Probability of observing data given H‚ÇÄ is true
- **Significance level (Œ±):** Threshold for rejection (typically 0.05)

**Decision rule:**
- If p < Œ± ‚Üí Reject H‚ÇÄ (significant difference)
- If p ‚â• Œ± ‚Üí Fail to reject H‚ÇÄ (no significant difference)

**In this project:**
```python
# Welch's t-test: Compare mean nominalization ratios
group_human = [0.25, 0.28, 0.23, ...]  # Human texts
group_ai = [0.42, 0.45, 0.40, ...]      # AI texts
t_stat, p_value = welch_ttest(group_human, group_ai)

if p_value < 0.05:
    print("AI texts have significantly different nominalization")
```

#### 2. Effect Size
**Definition:** Magnitude of difference between groups, independent of sample size.

**Why it matters:** p-values depend on sample size. With large N, tiny differences become "significant." Effect size tells you if the difference is **practically meaningful**.

**Cohen's d interpretation:**
- 0.2 = Small (barely noticeable)
- 0.5 = Medium (visible to careful observer)
- 0.8 = Large (obvious difference)
- 2.0+ = Very large (dramatic difference)

**Example from HC3 results:**
```
noun_ratio: Cohen's d = -4.65 (very large effect)
‚Üí AI uses dramatically more nouns than humans
```

#### 3. Multiple Comparisons Problem
**Problem:** Testing 12 metrics means 5% false positive rate per test ‚Üí 46% chance of at least one false positive!

**Solution:** FDR-BH (False Discovery Rate - Benjamini-Hochberg) correction
- Adjusts p-values to control false positives
- More powerful than Bonferroni correction
- Standard in genomics, neuroimaging, NLP

**Formula:**
```
For m tests, rank p-values: p‚ÇÅ ‚â§ p‚ÇÇ ‚â§ ... ‚â§ p‚Çò
Adjusted p-value: min(1, p·µ¢ √ó m / i)
```

**In this project:**
```python
raw_p_values = [0.044, 0.003, 0.021, ...]  # 12 tests
adjusted_p = adjust_pvalues(raw_p_values, method="fdr_bh")
# Now use adjusted_p for significance decisions
```

#### 4. Confidence Intervals
**Definition:** Range of values likely to contain the true population parameter.

**95% CI interpretation:**
- If we repeated the study 100 times, 95 CIs would contain the true mean difference
- Provides uncertainty quantification

**Example:**
```
Mean difference in noun_ratio: 0.085 [0.060, 0.110]
‚Üí We're 95% confident AI uses 6-11% more nouns
```

#### 5. Parametric vs. Non-parametric Tests
**Parametric (Welch's t-test):**
- Assumes approximately normal distribution
- More powerful when assumptions met
- Uses means

**Non-parametric (Mann-Whitney U):**
- No distribution assumptions
- Robust to outliers and skew
- Uses ranks (medians)

**Best practice:** Report both (as this project does)

---

### üìà Data Analysis Concepts

#### 6. Exploratory Data Analysis (EDA)
**Purpose:** Understand data before formal testing

**Steps in this project:**
1. **Summary statistics:** Mean, median, std, min, max
2. **Distribution plots:** Boxplots, histograms
3. **Correlation analysis:** Which features covary?
4. **Outlier detection:** Are there extreme values?

**Tools:**
```python
df.describe()  # Summary stats
df.groupby('label').mean()  # Compare groups
```

#### 7. Feature Engineering
**Definition:** Creating informative variables from raw data

**In this project:**
```
Raw data: "The implementation requires development."
    ‚Üì Tokenization
Tokens: ["implementation", "requires", "development"]
    ‚Üì POS tagging
POS: [NOUN, VERB, NOUN]
    ‚Üì Feature extraction
Features: {
    'word_count': 4,
    'noun_ratio': 0.5,
    'nominal_lemma_ratio': 0.67  # 2/3 nouns are nominalizations
}
```

**Why it matters:** Good features make or break analysis. Nominalization ratio is more informative than raw noun count.

#### 8. Dimensionality Reduction
**Problem:** 12+ features are hard to visualize

**Future work:** PCA or t-SNE to project to 2D
```
12 features ‚Üí 2 principal components
‚Üí Scatter plot showing human/AI clusters
```

#### 9. Cross-validation
**Purpose:** Assess generalization to new data

**Method:**
- K-fold CV: Split data into K parts, train on K-1, test on 1
- Repeat K times, average results
- Prevents overfitting

**Planned for ML integration:**
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print(f"Accuracy: {scores.mean():.3f} ¬± {scores.std():.3f}")
```

---

### üßÆ Statistical Metrics

#### 10. Accuracy, Precision, Recall, F1
**Context:** When doing binary classification (human vs. AI)

**Confusion matrix:**
```
                Predicted
              Human    AI
Actual Human    TP      FN
       AI       FP      TN
```

**Metrics:**
- **Accuracy** = (TP + TN) / Total
- **Precision** = TP / (TP + FP) ‚Äî "Of predicted humans, how many are correct?"
- **Recall** = TP / (TP + FN) ‚Äî "Of actual humans, how many did we catch?"
- **F1** = 2 √ó (Precision √ó Recall) / (Precision + Recall) ‚Äî Harmonic mean

**Trade-offs:**
- High precision ‚Üí Few false alarms
- High recall ‚Üí Few misses
- F1 ‚Üí Balance both

#### 11. ROC-AUC
**Purpose:** Evaluate classifier performance across thresholds

**ROC curve:**
- X-axis: False Positive Rate
- Y-axis: True Positive Rate
- Diagonal = random guessing

**AUC (Area Under Curve):**
- 0.5 = Random
- 0.7-0.8 = Good
- 0.9+ = Excellent

**Interpretation:** Probability that a random human text scores higher than a random AI text.

---

## 6. NLP Metrics in Research

### üî§ Lexical Metrics

#### Type-Token Ratio (TTR)
**Definition:** Vocabulary diversity measure

**Formula:**
```
TTR = |unique words| / |total words|
```

**Interpretation:**
- Low TTR (0.3-0.4): Repetitive language, limited vocabulary
- High TTR (0.6-0.8): Diverse vocabulary, varied expression

**Research findings:**
- Children: TTR ‚âà 0.4-0.5
- Adults: TTR ‚âà 0.5-0.6
- Academic writing: TTR ‚âà 0.6-0.7

**Limitation:** TTR decreases with text length (longer texts reuse words). Solution: Use first 100 words or standardized TTR (STTR).

**In AI detection:**
- Zhang (2024): No significant difference in TTR between human and AI
- **Interpretation:** AI has learned to vary vocabulary naturally

#### Average Sentence Length
**Definition:** Mean words per sentence

**Research correlations:**
- Short sentences (10-15 words): Journalistic, simple
- Medium sentences (15-20 words): Standard prose
- Long sentences (25+ words): Academic, complex

**In AI detection:**
- Hypothesis: AI might generate more uniform sentence lengths
- Finding: Mixed results across studies

#### Average Word Length
**Definition:** Mean characters per word

**Interpretation:**
- Short words (3-4 chars): Simple, common vocabulary
- Long words (6+ chars): Technical, sophisticated vocabulary

**In academic writing:** Longer words correlate with formality and technical content.

---

### üìù Syntactic Metrics

#### Nominalization Ratio ‚≠ê KEY METRIC
**Definition:** Proportion of nouns derived from verbs

**Formula:**
```
nominalization_ratio = verb-derived_nouns / total_nouns
```

**Linguistic significance:**
- **Process ‚Üí Entity:** "We analyzed" ‚Üí "The analysis"
- **Dynamic ‚Üí Static:** "Companies develop" ‚Üí "Corporate development"
- **Concrete ‚Üí Abstract:** "They decided" ‚Üí "The decision"

**Formality scale:**
- Conversational: Low nominalization (20-30%)
- Formal writing: Medium nominalization (30-40%)
- Academic/legal: High nominalization (40-50%+)

**Zhang (2024) finding:**
- **Human texts:** 32-38% nominalization
- **AI texts:** 42-48% nominalization
- **Conclusion:** AI writing is more formal/abstract than human academic writing

**Why AI over-nominalizes:**
1. **Training data:** Scientific papers are highly nominalized
2. **Pattern matching:** AI learns "analysis is..." is safer than "we analyzed..."
3. **Risk aversion:** Nominalization sounds more authoritative

**Examples:**
```
Human: "We developed a new method to analyze data."
AI: "The development of a novel methodology enables data analysis."
```

#### Part-of-Speech Ratios
**Noun ratio = nouns / total_words**
- High noun ratio ‚Üí Information-dense, formal
- AI tends toward higher noun ratio (47% vs 39% in HC3)

**Verb ratio = verbs / total_words**
- High verb ratio ‚Üí Action-oriented, dynamic
- Humans use more verbs (agency, narrative)

**Adjective ratio**
- Descriptive density
- Mixed findings across studies

**Interpretation:**
```
High nouns + Low verbs = Static, formal (AI tendency)
Low nouns + High verbs = Dynamic, active (Human tendency)
```

---

### üîó Collocation Metrics

#### Pointwise Mutual Information (PMI)
**Definition:** Measure of word association strength

**Formula:**
```
PMI(x,y) = log‚ÇÇ(P(x,y) / (P(x) √ó P(y)))

where:
    P(x,y) = count("x y") / total_bigrams
    P(x) = count("x") / total_words
```

**Interpretation:**
- **PMI > 0:** Words appear together more than random chance
- **PMI = 0:** Independent (no association)
- **PMI < 0:** Words avoid each other

**Examples:**
```
"neural network" ‚Üí PMI = 8.5 (very strong)
"the and" ‚Üí PMI = -2.3 (repulsion, grammatical artifacts)
```

**In AI detection:**
- AI may have stronger collocations (more formulaic)
- Humans have more varied pairings

#### N-grams
**Definition:** Sequences of N consecutive words

**Types:**
- Unigrams (N=1): Individual words
- Bigrams (N=2): Word pairs ("machine learning")
- Trigrams (N=3): 3-word sequences ("neural network architecture")

**Research use:**
- Identify formulaic language
- Detect plagiarism
- Language modeling

---

### üéØ Keyword Extraction

#### Log-Odds Ratio
**Definition:** Measure of word distinctiveness between corpora

**Formula:**
```
log_odds(w) = log‚ÇÇ((f_A + c) / (N_A + c√óV) / ((f_B + c) / (N_B + c√óV)))

where:
    f_A = frequency of word in corpus A
    N_A = total tokens in corpus A
    V = vocabulary size
    c = 0.5 (Haldane-Anscombe correction)
```

**Interpretation:**
- **Positive log-odds:** Distinctive of corpus A
- **Negative log-odds:** Distinctive of corpus B
- **Near zero:** Used equally in both

**Why Haldane-Anscombe correction (c=0.5)?**
- Prevents division by zero for absent words
- Smooths estimates for rare words
- Statistical best practice for log-odds

**Example from HC3:**
```
Human keywords: "feel", "think", "really", "just" (subjective, hedging)
AI keywords: "implementation", "analysis", "methodology" (formal, technical)
```

**Research applications:**
- Genre identification
- Author attribution
- Temporal change detection

---

### üìä Advanced NLP Concepts

#### Semantic Similarity
**Definition:** Meaning-based text comparison

**Methods:**
1. **Word embeddings (Word2Vec, GloVe):**
   - Vector representations of words
   - Cosine similarity in embedding space
   
2. **Sentence embeddings (BERT, Sentence-BERT):**
   - Contextual representations
   - Captures semantic relationships

**Future application:**
```python
# Measure semantic coherence within document
embeddings = model.encode(sentences)
coherence = mean(cosine_similarity(embeddings[i], embeddings[i+1]))
‚Üí AI might have lower inter-sentence coherence
```

#### Topic Modeling
**Definition:** Discover latent themes in text collections

**Methods:**
- **LDA (Latent Dirichlet Allocation):** Probabilistic topic discovery
- **NMF (Non-negative Matrix Factorization):** Linear algebra approach

**Hypothesis:** AI might have less topic focus or more generic topics.

#### Dependency Parsing
**Definition:** Analyze grammatical relationships between words

**Parse tree depth:**
- Shallow trees ‚Üí Simple sentences
- Deep trees ‚Üí Complex subordination

**Research question:** Does AI generate more uniform syntactic structures?

#### Perplexity
**Definition:** How "surprised" a language model is by text

**Formula:**
```
perplexity = 2^(-log‚ÇÇP(text))
```

**Interpretation:**
- Low perplexity ‚Üí Text is predictable (formulaic)
- High perplexity ‚Üí Text is surprising (creative)

**Hypothesis:** AI-generated text has lower perplexity when scored by the same model family.

---

## 7. Statistical Methods Deep Dive

### Why These Tests?

#### Welch's t-test vs. Student's t-test
**Student's t-test assumption:** Equal variances (homoscedasticity)
- Problematic if œÉ¬≤_A ‚â† œÉ¬≤_B

**Welch's t-test:**
- Robust to unequal variances
- Adjusts degrees of freedom (Welch-Satterthwaite)
- **Always preferred for real-world data**

**This project uses Welch's** because human and AI texts may have different variability.

#### Parametric vs. Non-parametric
**When to use Mann-Whitney instead of t-test:**
1. Small sample sizes (n < 30)
2. Heavily skewed distributions
3. Outliers present
4. Ordinal data (ranks instead of intervals)

**Best practice:** Report both (as this project does)
- If results agree ‚Üí Robust finding
- If results differ ‚Üí Distribution issues, investigate further

#### Why FDR-BH Correction?
**Problem:** Testing 12 metrics ‚Üí 12 chances for false positives

**Options:**
1. **Bonferroni:** Œ±_adjusted = 0.05 / 12 ‚âà 0.004 (very conservative, low power)
2. **FDR-BH:** Controls expected proportion of false positives (more powerful)

**FDR-BH advantage:**
- Allows more discoveries than Bonferroni
- Controls false discovery rate (not family-wise error rate)
- Standard in high-dimensional testing (genomics, neuroimaging)

**Example:**
```
12 raw p-values: [0.001, 0.003, 0.044, 0.087, ...]
After FDR-BH: [0.012, 0.018, 0.176, 0.261, ...]
‚Üí First 2 remain significant at Œ±=0.05
```

---

## 8. Quick Reference

### Common Commands

```bash
# Basic run
python run.py

# CLI with custom data
python scripts/analyze_nominalization.py --input data.csv --outdir results/

# Run tests
pytest tests/

# Check code style
black src/ scripts/ tests/
flake8 src/ scripts/

# Install in dev mode
pip install -e .[dev]
```

### File Locations Cheat Sheet

```
Input data:     data/raw/sample_data.csv
Output CSV:     results/human_vs_ai_augmented.csv
Figures:        results/figures/*.png
Statistics:     results/tables/statistical_tests.csv
Keywords:       results/tables/keywords_group_*.csv
Main script:    run.py
CLI:            scripts/analyze_nominalization.py
Core logic:     src/run_pipeline.py
Tests:          tests/test_*.py
```

### Interpretation Guidelines

**p-value < 0.05:** Significant difference  
**Cohen's d:**
- < 0.2: Negligible
- 0.2-0.5: Small
- 0.5-0.8: Medium
- \> 0.8: Large

**Nominalization ratio:**
- < 0.3: Informal
- 0.3-0.4: Standard academic
- \> 0.4: Highly formal

**TTR:**
- < 0.5: Repetitive
- 0.5-0.6: Normal
- \> 0.6: Diverse

---

## Key Takeaways

### For Researchers
1. This pipeline provides **rigorous, reproducible** linguistic analysis
2. Nominalization ratio is a **strong signal** for AI detection
3. Always report **both p-values and effect sizes**
4. Use **FDR correction** for multiple comparisons

### For Developers
1. **Modular design** makes extending easy
2. **Batch processing** is critical for performance
3. **Fallback strategies** improve robustness
4. **Comprehensive testing** prevents regressions

### For Data Scientists
1. **Feature engineering** is more important than model choice
2. **Statistical rigor** builds trust in findings
3. **Visualization** communicates results effectively
4. **Documentation** enables reproducibility

---

## References

**Original Study:**
- Zhang, M. (2024). "More Human Than Human? Investigating ChatGPT's Linguistic Footprints on Academic Writing." *IRAL - International Review of Applied Linguistics in Language Teaching*. De Gruyter.

**Statistical Methods:**
- Benjamini, Y., & Hochberg, Y. (1995). "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing." *Journal of the Royal Statistical Society: Series B*, 57(1), 289-300.

**NLP Resources:**
- spaCy: https://spacy.io/
- NLTK: https://www.nltk.org/
- WordNet: https://wordnet.princeton.edu/

---

**For questions or contributions:** https://github.com/nguyendinhthienloc/SC203

**License:** MIT
